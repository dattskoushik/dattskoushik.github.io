<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0">
    <title>Designing Reliable Batch Data Pipelines | Suchindra Koushik</title>
    <meta name="description" content="Building a script to move data from A to B is easy. Building a system that moves data from A to B *reliably*, at scale, while handling network failures, bad ...">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../css/main.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <script src="../assets/js/theme.js"></script>
</head>
<body>

    <header class="site-header">
        <div class="container">
            <a href="../" class="logo">Suchindra Koushik</a>
            <button class="mobile-menu-btn" aria-label="Menu" aria-expanded="false">☰</button>
            <nav class="nav-links">
                <a href="../">Home</a>
                <a href="index.html" class="active">Writing</a>
                <a href="../projects.html">Projects</a>
                <a href="../about.html">About</a>
                <a href="../resume.html">Resume</a>
                <button id="theme-toggle" aria-label="Toggle theme">☀️</button>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="hero" style="padding-bottom: var(--space-md); max-width: 800px; margin: 0 auto;">
            <a href="index.html" style="font-size: 0.875rem; color: var(--text-muted); text-decoration: none; display: inline-block; margin-bottom: var(--space-sm);">&larr; Back to Writing</a>
            <div class="hero-meta" style="margin-top: 0; margin-bottom: var(--space-sm);">
                Architecture • September 2023
            </div>
            <h1 style="margin-bottom: var(--space-md);">Designing Reliable Batch Data Pipelines</h1>
        </article>

        <div class="post-content">
            <p>Building a script to move data from A to B is easy. Building a system that moves data from A to B <em>reliably</em>, at scale, while handling network failures, bad data, and restarts without corruption? That is engineering.</p>
<p>In the world of batch processing (ETL), "Reliability" usually boils down to two properties: <strong>Resilience</strong> (not crashing when things go wrong) and <strong>Correctness</strong> (not duplicating or losing data when things go wrong).</p>
<p>This article explores the patterns required to sleep well at night while your pipelines run.</p>
<h2>The Fallacy of "Retry Loop"</h2>
<p>The naive approach to reliability is the retry loop.</p>
<pre><code class="language-python">def process_data(batch):
    try:
        insert_to_db(batch)
    except Exception:
        time.sleep(5)
        process_data(batch)  # Retry
</code></pre>
<p>This works for transient network blips. But what if the failure is caused by a "Poison Pill"—a specific record that triggers a parser error? Infinite retries. The pipeline stalls.</p>
<p>What if the failure happens <em>after</em> insertion but <em>before</em> acknowledgement? You retry and insert the data twice. Duplicate data.</p>
<h2>1. Idempotency: The Golden Rule</h2>
<p><strong>Idempotency</strong> means that applying an operation multiple times has the same result as applying it once. <code>f(f(x)) = f(x)</code>.</p>
<p>In data pipelines, this allows us to "replay" failed batches safely.</p>
<h3>Database Level Idempotency</h3>
<p>The most robust way to ensure idempotency is relying on the destination's primary key constraints.</p>
<pre><code class="language-sql">INSERT INTO daily_metrics (date, metric_id, value)
VALUES ('2023-10-27', 'active_users', 4500)
ON CONFLICT (date, metric_id)
DO UPDATE SET value = EXCLUDED.value;
</code></pre>
<p>This "Upsert" ensures that if we re-run the job for '2023-10-27', we verify the state rather than blindly appending.</p>
<h3>File Level Idempotency</h3>
<p>When writing to object storage (S3), we cannot "update" files. We must rely on deterministic naming.</p>
<p><strong>Bad:</strong> <code>s3://bucket/data/upload_timestamp.json</code> (Non-deterministic)
<strong>Good:</strong> <code>s3://bucket/data/partition_date/batch_id.json</code> (Deterministic)</p>
<h2>2. Checkpointing and State Management</h2>
<p>For long-running batch jobs (e.g., processing a huge log file), you cannot afford to restart from line 1 if it fails at line 999,999. You need <strong>Checkpointing</strong>.</p>
<p>We can maintain a separate state store (Redis, DynamoDB, or a Postgres table) to track progress.</p>
<pre><code class="language-python"># Simplified Logic
last_processed_id = get_checkpoint(&quot;job_123&quot;)
records = fetch_source(after=last_processed_id)

for batch in chunk(records):
    process(batch)
    save_checkpoint(&quot;job_123&quot;, batch.last_id)
</code></pre>
<p>However, there is a race condition here. If <code>process(batch)</code> succeeds (writes to DB) but <code>save_checkpoint</code> fails (network error), the next run will re-process the batch.
<strong>Solution:</strong> Atomic Commit. If the destination supports transactions (like Postgres), write the data AND the checkpoint in the same transaction.</p>
<h2>3. The Dead Letter Queue (DLQ)</h2>
<p>What do you do with bad data? If 1 row in a batch of 10,000 is malformed, should you fail the whole batch? Usually, no.</p>
<p>The pattern is to <strong>quarantine</strong> the bad data.</p>
<pre><code class="language-python">def process_batch(records):
    valid_records = []
    failed_records = []

    for record in records:
        try:
            validate(record)
            valid_records.append(record)
        except ValidationError as e:
            failed_records.append({&quot;data&quot;: record, &quot;error&quot;: str(e)})

    bulk_insert(valid_records)
    send_to_dlq(failed_records)
</code></pre>
<p>The DLQ (Dead Letter Queue) can be an S3 bucket or a separate queue. It allows the pipeline to proceed while preserving the evidence for debugging.</p>
<h2>Code Example: Robust Log Ingestion</h2>
<p>Let's look at a snippet inspired by a log ingestion system I built (see full code in <code>assets/code_reference/log-parser</code>).</p>
<p>We use <strong>Pydantic</strong> for strict validation at the edge. This ensures that "poison pills" are caught early, before they enter the database.</p>
<pre><code class="language-python">from pydantic import BaseModel, Field, ValidationError
from datetime import datetime
from enum import Enum

class LogSeverity(str, Enum):
    INFO = 'INFO'
    WARN = 'WARN'
    ERROR = 'ERROR'

class LogEntry(BaseModel):
    timestamp: datetime
    severity: LogSeverity
    module: str = Field(min_length=1)
    message: str

def parse_and_quarantine(raw_logs):
    valid = []
    quarantine = []

    for log_line in raw_logs:
        try:
            # Pydantic handles type coercion and validation
            parsed = parse_regex(log_line)
            entry = LogEntry(**parsed)
            valid.append(entry.dict())
        except ValidationError as e:
            quarantine.append({&quot;raw&quot;: log_line, &quot;error&quot;: e.errors()})

    return valid, quarantine
</code></pre>
<p>By segregating the data immediately, we protect the downstream analytical storage (Warehouse) from schema violations.</p>
<h2>4. Backfilling: The Time Travel Problem</h2>
<p>Pipelines don't just process <em>new</em> data. Often, business logic changes ("We need to calculate the metric differently"). You need to <strong>Backfill</strong> historical data.</p>
<p>This is where Idempotency pays off. If your pipeline is idempotent, a backfill is just "running the job for past dates".</p>
<p><strong>Key Strategy:</strong> Decouple "Execution Time" from "Data Time".
Never use <code>datetime.now()</code> inside your transformation logic. Always pass the <code>data_date</code> as a parameter.</p>
<pre><code class="language-python"># BAD
def calculate_daily_revenue():
    today = datetime.now().date()
    ...

# GOOD
def calculate_daily_revenue(target_date: date):
    ...
</code></pre>
<p>This allows you to run <code>calculate_daily_revenue('2023-01-01')</code> regardless of the actual wall-clock time.</p>
<h2>Conclusion</h2>
<p>Reliable pipelines are boring. They don't crash on bad input; they just sidebar it. They don't double-count data on retries; they upsert. They don't fear restarts; they checkpoint.</p>
<p>As engineers, our job is to anticipate failure. By designing for idempotency and observability (DLQs), we build systems that recover automatically, allowing us to focus on the next interesting problem.</p>
        </div>
    </main>

    <footer class="site-footer container">
        <p class="copyright">&copy; <script>document.write(new Date().getFullYear())</script> Suchindra Koushik. All rights reserved.</p>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-go.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-rust.min.js"></script>
</body>
</html>
